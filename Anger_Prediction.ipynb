{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Importing Libraries"],"metadata":{"id":"1eCJwErpuO0z"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PGSNxyOZubq_","executionInfo":{"status":"ok","timestamp":1662416230916,"user_tz":-360,"elapsed":24886,"user":{"displayName":"Ahmed Rafi Hasan","userId":"13961549991423405694"}},"outputId":"dba94e98-69c6-435f-8d32-663715db1a70"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","import os\n","import sys\n","\n","# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n","import librosa\n","import librosa.display\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.model_selection import train_test_split\n","\n","# to play the audio files\n","from IPython.display import Audio\n","\n","import keras\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.utils import to_categorical\n","from keras.callbacks import ModelCheckpoint\n","\n","import warnings\n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "],"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"uXDGGp35uO00","executionInfo":{"status":"ok","timestamp":1662416289855,"user_tz":-360,"elapsed":446,"user":{"displayName":"Ahmed Rafi Hasan","userId":"13961549991423405694"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Data Preparation\n","* As we are working with four different datasets, so i will be creating a dataframe storing all emotions of the data in dataframe with their paths.\n","* We will use this dataframe to extract features for our model training."],"metadata":{"id":"4UVoLzoouO01"}},{"cell_type":"code","source":["# Paths for data.\n","\n","Dataset = \"/content/drive/MyDrive/DataMining/Datasets\"\n"],"metadata":{"trusted":true,"id":"UZOqh7QGuO02","executionInfo":{"status":"ok","timestamp":1662416419920,"user_tz":-360,"elapsed":389,"user":{"displayName":"Ahmed Rafi Hasan","userId":"13961549991423405694"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Data Augmentation\n","\n","- Data augmentation is the process by which we create new synthetic data samples by adding small perturbations on our initial training set.\n","- To generate syntactic data for audio, we can apply noise injection, shifting time, changing pitch and speed.\n","- The objective is to make our model invariant to those perturbations and enhace its ability to generalize.\n","- In order to this to work adding the perturbations must conserve the same label as the original training sample.\n","- In images data augmention can be performed by shifting the image, zooming, rotating ...\n","\n","First, let's check which augmentation techniques works better for our dataset."],"metadata":{"id":"12L0RUm1uO09"}},{"cell_type":"code","source":["def noise(data):\n","    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n","    data = data + noise_amp*np.random.normal(size=data.shape[0])\n","    return data\n","\n","def stretch(data, rate=0.8):\n","    return librosa.effects.time_stretch(data, rate)\n","\n","def shift(data):\n","    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n","    return np.roll(data, shift_range)\n","\n","def pitch(data, sampling_rate, pitch_factor=0.7):\n","    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n","\n","# taking any example and checking for techniques.\n","path = np.array(data_path.Path)[1]\n","data, sample_rate = librosa.load(path)"],"metadata":{"trusted":true,"id":"UyvmwiA1uO0-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_features(data):\n","    # ZCR\n","    result = np.array([])\n","    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n","    result=np.hstack((result, zcr)) # stacking horizontally\n","\n","    # Chroma_stft\n","    stft = np.abs(librosa.stft(data))\n","    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n","    result = np.hstack((result, chroma_stft)) # stacking horizontally\n","\n","    # MFCC\n","    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n","    result = np.hstack((result, mfcc)) # stacking horizontally\n","\n","    # Root Mean Square Value\n","    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n","    result = np.hstack((result, rms)) # stacking horizontally\n","\n","    # MelSpectogram\n","    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n","    result = np.hstack((result, mel)) # stacking horizontally\n","    \n","    return result\n","\n","def get_features(path):\n","    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n","    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n","    \n","    # without augmentation\n","    res1 = extract_features(data)\n","    result = np.array(res1)\n","    \n","    # data with noise\n","    noise_data = noise(data)\n","    res2 = extract_features(noise_data)\n","    result = np.vstack((result, res2)) # stacking vertically\n","    \n","    # data with stretching and pitching\n","    new_data = stretch(data)\n","    data_stretch_pitch = pitch(new_data, sample_rate)\n","    res3 = extract_features(data_stretch_pitch)\n","    result = np.vstack((result, res3)) # stacking vertically\n","    \n","    return result"],"metadata":{"trusted":true,"id":"laD2O5JruO1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, Y = [], []\n","for path, emotion in zip(data_path.Path, data_path.Emotions):\n","    feature = get_features(path)\n","    for ele in feature:\n","        X.append(ele)\n","        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n","        Y.append(emotion)"],"metadata":{"trusted":true,"id":"zCLwe03fuO1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(X), len(Y), data_path.Path.shape"],"metadata":{"trusted":true,"id":"8PefVm1FuO1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Features = pd.DataFrame(X)\n","Features['labels'] = Y\n","Features.to_csv('features.csv', index=False)\n","Features.head()"],"metadata":{"trusted":true,"id":"1eDmywbHuO1B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* We have applied data augmentation and extracted the features for each audio files and saved them."],"metadata":{"id":"xhtKwPcEuO1B"}},{"cell_type":"markdown","source":["## Data Preparation\n","\n","- As of now we have extracted the data, now we need to normalize and split our data for training and testing."],"metadata":{"id":"FaOTbIG9uO1B"}},{"cell_type":"code","source":["X = Features.iloc[: ,:-1].values\n","Y = Features['labels'].values"],"metadata":{"trusted":true,"id":"VMZm4IxvuO1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# As this is a multiclass classification problem onehotencoding our Y.\n","encoder = OneHotEncoder()\n","Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"],"metadata":{"trusted":true,"id":"TBH7Mtu5uO1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# splitting data\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"],"metadata":{"trusted":true,"id":"5MqOJz0SuO1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scaling our data with sklearn's Standard scaler\n","scaler = StandardScaler()\n","x_train = scaler.fit_transform(x_train)\n","x_test = scaler.transform(x_test)\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"],"metadata":{"trusted":true,"id":"1ehpCa8guO1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# making our data compatible to model.\n","x_train = np.expand_dims(x_train, axis=2)\n","x_test = np.expand_dims(x_test, axis=2)\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"],"metadata":{"trusted":true,"id":"F-iJ2RgquO1D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Modelling"],"metadata":{"id":"Ec0X15b_uO1D"}},{"cell_type":"code","source":["model=Sequential()\n","model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n","\n","model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n","\n","model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n","\n","model.add(Flatten())\n","model.add(Dense(units=32, activation='relu'))\n","model.add(Dropout(0.3))\n","\n","model.add(Dense(units=8, activation='softmax'))\n","model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n","\n","model.summary()"],"metadata":{"trusted":true,"id":"90sk7X_wuO1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n","history=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])"],"metadata":{"trusted":true,"id":"UKC2wjw-uO1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n","\n","epochs = [i for i in range(50)]\n","fig , ax = plt.subplots(1,2)\n","train_acc = history.history['accuracy']\n","train_loss = history.history['loss']\n","test_acc = history.history['val_accuracy']\n","test_loss = history.history['val_loss']\n","\n","fig.set_size_inches(20,6)\n","ax[0].plot(epochs , train_loss , label = 'Training Loss')\n","ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n","ax[0].set_title('Training & Testing Loss')\n","ax[0].legend()\n","ax[0].set_xlabel(\"Epochs\")\n","\n","ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n","ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n","ax[1].set_title('Training & Testing Accuracy')\n","ax[1].legend()\n","ax[1].set_xlabel(\"Epochs\")\n","plt.show()"],"metadata":{"trusted":true,"id":"bl5wVYX0uO1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predicting on test data.\n","pred_test = model.predict(x_test)\n","y_pred = encoder.inverse_transform(pred_test)\n","\n","y_test = encoder.inverse_transform(y_test)"],"metadata":{"trusted":true,"id":"gk5fl0I2uO1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n","df['Predicted Labels'] = y_pred.flatten()\n","df['Actual Labels'] = y_test.flatten()\n","\n","df.head(10)"],"metadata":{"trusted":true,"id":"WJCjjOyguO1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cm = confusion_matrix(y_test, y_pred)\n","plt.figure(figsize = (12, 10))\n","cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n","sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n","plt.title('Confusion Matrix', size=20)\n","plt.xlabel('Predicted Labels', size=14)\n","plt.ylabel('Actual Labels', size=14)\n","plt.show()"],"metadata":{"trusted":true,"id":"_F5FNx4tuO1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_test, y_pred))"],"metadata":{"trusted":true,"id":"FfTVvwcyuO1E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- We can see our model is more accurate in predicting surprise, angry emotions and it makes sense also because audio files of these emotions differ to other audio files in a lot of ways like pitch, speed etc..\n","- We overall achieved 61% accuracy on our test data and its decent but we can improve it more by applying more augmentation techniques and using other feature extraction methods."],"metadata":{"id":"gNnJM5r9uO1E"}},{"cell_type":"markdown","source":["### This is all i wanna do in this project. Hope you guyz like this. \n","### If you like the kernel make sure to upvote it please :-)"],"metadata":{"id":"vUfOm_ZhuO1E"}}]}